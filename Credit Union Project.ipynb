{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canadian Credit Union Yelp and Asset Growth Project\n",
    "\n",
    "Here we explore the correlation between Yelp! reviews and asset growth using various recurrent neural networks. From our experimentation with the data, and features, we found _ yielded the best accuracy of future asset growth. This prediction was generated using Yelp! reviews and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-Up\n",
    "\n",
    "Here we will import the necessary libraries that we will need for the project. Additionally, will read in the data collected from various Canadian Credit Unions and their corresponding Yelp! reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first start by importing the libraries and data we'll need\n",
    "# Libraries needed include numpy, keras, csv, matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Need to first import the data into a numpy array so we can do some work with it\n",
    "\n",
    "fname = 'data'\n",
    "f = open(fname)\n",
    "data = f.read();\n",
    "f.close\n",
    "\n",
    "# Now need to separate the column headers from the rest of the data\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "Here we will be doing the data cleaning, parsing and preparation. This will involve parsing out symbols and punctuation that may interfere with the model's ability to analyze the given data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization\n",
    "\n",
    "Below we will be vectorizing the Yelp reviews for the various credit unions. Please refer to Google Document for list of urls utilized in the collection of the Yelp Reviews. Will vectorize data, with 1- and 2- grams.\n",
    "\n",
    "Here we are importing a useful function that converts our data set of sentences into a large matrix of 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data vectorization code can be found below \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# The matrix has each row representing a sentence and each column representing a word from the data set (no words are repeated). For each sentence, a 1 is placed in the columns of the words present in the sentence. If a word isn't in the sentence, a 0 is put in that column.\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, lowercase=False)\n",
    "# 'binary=false' would make the matrix count the frequency of a word in the sentence, instead of just marking its presence\n",
    "# for some reason this wasn't working unless lowercase=false, some problem with the cleaned data I suppose\n",
    "# This next line could be used instead if we wished to make the program more sophisticated, but larger/slower. Instead of a column for every word, there would also be columns for every set of two words placed next to each other in the data set. You can change the 2 to any integer, but it makes the matrix exponentially larger.\n",
    "\n",
    "#vectorizer = CountVectorizer(binary=True, lowercase=False, ngram_range=(1, 2))\n",
    "# Finally, we can simply call this function on our cleaned dataset 'phrases'.\n",
    "\n",
    "vector = vectorizer.fit_transform(review)\n",
    "# To use this with keras we need to first convert it to a numpy array.\n",
    "\n",
    "# Change to a numpy array\n",
    "\n",
    "data = vector.todense()\n",
    "data = np.asarray(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data up into the training, validation, and testing sets, at a 60:20:20 ratio. We are careful to have an even number of positive and negative sentiments in each section. Without staying even, our model would learn to guess, say, positive more often, simply because more reviews were positive!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, test, and validate sets\n",
    "\n",
    "x_train = np.concatenate([data[:300], data[-300:]])\n",
    "y_train = np.concatenate([sentiment[:300], sentiment[-300:]])\n",
    "x_val = np.concatenate([data[300:400], data[600:700]])\n",
    "y_val = np.concatenate([sentiment[300:400], sentiment[600:700]])\n",
    "x_test = np.concatenate([data[400:600]])\n",
    "y_test = np.concatenate([sentiment[400:600]])\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Prediction \n",
    "\n",
    "Will now import the model we developed during our warm-up project for the purposes of predicting the sentiment of credit union reviews. This can be done now that we have cleaned and vectorized our data. Since keras is wonderful and has a built-in model prediction feature, won't unnecessarily complicate the prediction process by re-inventing the wheel. But rather, will simply pass in our new data into the built in prediction feature in keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to load our model into the program, can be done using the model.load(\"Model name.h5\") feature\n",
    "\n",
    "model.load(\"Model Name.h5\")\n",
    "\n",
    "# Now make our prediction using the new data set to see what the sentiment of our credit union reviews is like\n",
    "\n",
    "sentiment = model.predict(x_train, y_train)\n",
    "\n",
    "# Now print out our findings from the sentiment analysis conducted above, see what the sentiment of \n",
    "# the credit union reviews looks like\n",
    "\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics \n",
    "\n",
    "Here we will plot some of our data to see if we can see any obvious patterns. Best to do that before going right into the model creation so that we can ensure that the obvious patterns are indeed accounted for during that stage. Here will simply look at temperature versus time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our Vancity asset data into a numpy array for convenience\n",
    "\n",
    "vassets  = float_data[:,1] \n",
    "time  = data[:,1]\n",
    "plt.plot(time, data, label = 'Asset Growth ($ CAD)')\n",
    "plt.title('Asset Growth By Year')\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Asset Growth ($ CAD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the plot above, we should be expecting year over year increase in assets. This will provide us with a diagnostic to ensure the model is providing us with reasonable output. Additionally, should get yearly growths in excess of a few hundreds of millions of dollars. This should pick up to a few billion dollars around the year 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Finally, after all the hard work of normalizing our data, taking a quick look at it, we get the to fun part: model creation. Will be taking Yelp! reviews and provided asset growth data to predict future asset growth as our only output. This will be accomplished using an LSTM recurrent neural network model with _ layers and _ neurons. These were selected as they yielded the highest accuracy from our experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Randy_B15\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lookback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bf43b5a8192f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Add some layers to our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlookback\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lookback' is not defined"
     ]
    }
   ],
   "source": [
    "# Need to import some libraries from keras to create our model\n",
    "# This will involve the use of keras sequential neural network models, layers and rmsprop optimizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "# Now need to define how far back into the data set wqe wish to go, how often we wish to sample,\n",
    "# and how far into the future we wish to predict \n",
    "\n",
    "# Will use all of our data to make future predictions, as such, will use length of time series column \n",
    "# since each of the credit unions gave variable historical data (i.e. did not share common report start date)\n",
    "\n",
    "lookback  = len(time)\n",
    "\n",
    "# Since we have limited annual data, will sample every year, therefore will use a step of 1\n",
    "\n",
    "step = 1\n",
    "\n",
    "# Will start by predicting asset growth one year into the future, and will extend outwards from there\n",
    "\n",
    "delay  = 1\n",
    "\n",
    "# The rest is very similar to the creation of the sequential neural network we made during the Warm-Up project\n",
    "# Quick refresher though, need to define our model with a number of layers, an optimzer function, a loss function, an activation function, and how many layers we want it to be\n",
    "\n",
    "# Define our model as a sequential one\n",
    "model = Sequential()\n",
    "\n",
    "# Add some layers to our model \n",
    "model.add(LSTM(32, input_shape=(x_train.shape[1:], y_train.shape[1:]), return_sequences = True))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Now compile our model with optimizer and loss functions, no metric for this one though\n",
    "model.compile(optimizer = RMSprop(), loss = 'mae')\n",
    "history =  model.fit_generator(x_train,\n",
    "                               y_train,\n",
    "                               steps_per_epoch = 5,\n",
    "                               epochs = 1,\n",
    "                               validation_data = (x_val, y_val))\n",
    "\n",
    "# Before we go any further, some important notes to make here. Will do that below in the \"Model Notes\" block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Notes\n",
    "\n",
    "1. Could have used other activation functions, 'relu' is a pretty popular one, but could use the likes of 'selu' and 'sigmoid'.\n",
    "\n",
    "2. The number of layers we add is completely arbitrary and is usually driven by experimenting with the model to see what works the best for the project.\n",
    "\n",
    "3. The number of epochs is another great place to play around. This is primarily due to wanting to avoid overfitting, which can happen by having too many training epochs. As such, should play around and see how many epochs yields the best result for the model.\n",
    "\n",
    "4. The optimizer function is another area to play around as RMSprop may not always be the best choice for the project at hand.\n",
    "\n",
    "5. The loss function selected here was another judgement call, but others could be used such as binary cross entropy. Used here since we actually have numbers to match to our model's prediction, so makes sense to use mean absolute error to see how far away our model's predictions are so we can mitigate the errors. Could also use root mean square method as well for the same purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Output\n",
    "\n",
    "Will now plot our model's prediction against the actual data, and validation data to see if we're overfitting, and how our model is performing overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will grab our losses by going into the training history and defining appropriate variables to make plotting easier\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss - history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Now plot our training and validation losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label = 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Traing and Validation Losses By Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Will now have our model make a prediction\n",
    "# X will be our input that we give to our model to make its prediction\n",
    "\n",
    "prediction = model.predict(X)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "As is evident from the plot and historical output above, our model is able to achieve accuracy in the range of _%-_%. Furthermore, it is evident that in using _ epochs, the model is able to avoid overfitting to the training data. As such, it can be concluded that our model is sufficiently effective in predicting future asset growth for Canadian Credit Unions using Yelp! reviews, and yearly asset growth. Lastly, it can be concluded that Vancity Credit Uni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
