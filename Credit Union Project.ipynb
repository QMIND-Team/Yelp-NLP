{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canadian Credit Union Yelp and Asset Growth Project\n",
    "\n",
    "Here we explore the correlation between Yelp! reviews and asset growth using various recurrent neural networks. From our experimentation with the data, and features, we found _ yielded the best accuracy of future asset growth. This prediction was generated using Yelp! reviews and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-Up\n",
    "\n",
    "Here we will import the necessary libraries that we will need for the project. Additionally, will read in the data collected from various Canadian Credit Unions and their corresponding Yelp! reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first start by importing the libraries and data we'll need\n",
    "# Libraries needed include numpy, keras, csv, matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Import data\n",
    "\n",
    "data = pickle.load(open(\"dfFinal.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "Here we will be doing the data cleaning, parsing and preparation. This will involve parsing out symbols and punctuation that may interfere with the model's ability to analyze the given data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "\n",
    "To ensure the data fits with our model, will define a function that will put the data into a model friendly shape and form. This will then be called prior to model compilation and training with a defined lookback period, step size, batch size and delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will define a function with all the inputs we'll need for our recurrent neural network as it changes the data given to it \n",
    "# This function will need our normalized data array, our delay, min and max indices, shuffle command (do we want to randomly draw on the data each time or not?), the batch size of our data and the number of steps\n",
    "\n",
    "def generator(data, lookback, delay , min_index, max_index, shuffle = False, batch_size = 128, step = 6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data)  - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size = batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows), ))\n",
    "        \n",
    "        for j , row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics \n",
    "\n",
    "Here we will plot some of our data to see if we can see any obvious patterns. Best to do that before going right into the model creation so that we can ensure that the obvious patterns are indeed accounted for during that stage. Here will simply look at temperature versus time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our Vancity asset data into a numpy array for convenience\n",
    "\n",
    "vassets  = float_data[:,1] \n",
    "time  = data[:,1]\n",
    "plt.plot(time, data, label = 'Asset Growth ($ CAD)')\n",
    "plt.title('Asset Growth By Year')\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Asset Growth ($ CAD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the plot above, we should be expecting year over year increase in assets. This will provide us with a diagnostic to ensure the model is providing us with reasonable output. Additionally, should get yearly growths in excess of a few hundreds of millions of dollars. This should pick up to a few billion dollars around the year 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the generator.  Hardcoded for our purpose\n",
    "\n",
    "def generator(data, lookback, delay , min_index, max_index, batch_size = 128):\n",
    "    step = 1\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data)  - delay - 1\n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index + lookback\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows), ))\n",
    "        \n",
    "        for j , row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Finally, after all the hard work of normalizing our data, taking a quick look at it, we get the to fun part: model creation. Will be taking Yelp! reviews and provided asset growth data to predict future asset growth as our only output. This will be accomplished using an LSTM recurrent neural network model with _ layers and _ neurons. These were selected as they yielded the highest accuracy from our experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Randy_B15\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lookback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bf43b5a8192f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Add some layers to our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlookback\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lookback' is not defined"
     ]
    }
   ],
   "source": [
    "# Need to import some libraries from keras to create our model\n",
    "# This will involve the use of keras sequential neural network models, layers and rmsprop optimizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "\n",
    "# Now need to define how far back into the data set wqe wish to go, how often we wish to sample,\n",
    "# and how far into the future we wish to predict \n",
    "\n",
    "# Will use all of our data to make future predictions, as such, will use length of time series column \n",
    "# since each of the credit unions gave variable historical data (i.e. did not share common report start date)\n",
    "\n",
    "lookback  = len(time)\n",
    "\n",
    "# Since we have limited annual data, will sample every year, therefore will use a step of 1\n",
    "\n",
    "step = 1\n",
    "\n",
    "# Will now define a batch size for our model so wqe can call the data genertator function\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "# Will start by predicting asset growth one year into the future, and will extend outwards from there\n",
    "\n",
    "delay  = 1\n",
    "\n",
    "# Get our data using the data generator function\n",
    "\n",
    "# Will start with the training data set, seems like an obvious place to start\n",
    "\n",
    "train_gen =  generator(float_data, lookback=lookback, delay=delay, min_index=0,max_index=(number),shuffle=True,step=step,batch_size=batch_size)\n",
    "\n",
    "# Will then create a validation data set using the exact same parameters, but shifting our indices up\n",
    "\n",
    "val_gen =  generator(float_data, lookback=lookback, delay=delay, min_index=200001,max_index=300000,shuffle=False,step=step,batch_size=batch_size)\n",
    "\n",
    "# And lastly will generate a testing data set \n",
    "\n",
    "test_gen =  generator(float_data, lookback=lookback, delay=delay, min_index=300001,max_index=None,shuffle=False,step=step,batch_size=batch_size)\n",
    "\n",
    "#Set how many steps we need to get the entire validation data set\n",
    "#val_steps = (300000 - 200001 - lookback)\n",
    "\n",
    "#Set how many steps we'll need to get the entire testing data set\n",
    "#test_steps = (len(float_data) -  300001 - lookback)\n",
    "\n",
    "# The rest is very similar to the creation of the sequential neural network we made during the Warm-Up project\n",
    "# Quick refresher though, need to define our model with a number of layers, an optimzer function, a loss function, an activation function, and how many layers we want it to be\n",
    "\n",
    "# Define our model as a sequential one\n",
    "model = Sequential()\n",
    "\n",
    "# Add some layers to our model \n",
    "model.add(LSTM(32, input_shape=(lookback // step, float_data.shape[-1]), return_sequences = True))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Now compile our model with optimizer and loss functions, no metric for this one though\n",
    "model.compile(optimizer = RMSprop(), loss = 'mae')\n",
    "history =  model.fit_generator(train_gen,\n",
    "                               steps_per_epoch = 50,\n",
    "                               epochs = 1,\n",
    "                               validation_data = val_gen,\n",
    "                               validation_steps = step)\n",
    "\n",
    "# Before we go any further, some important notes to make here. Will do that below in the \"Model Notes\" block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Notes\n",
    "\n",
    "1. Could have used other activation functions, 'relu' is a pretty popular one, but could use the likes of 'selu' and 'sigmoid'.\n",
    "\n",
    "2. The number of layers we add is completely arbitrary and is usually driven by experimenting with the model to see what works the best for the project.\n",
    "\n",
    "3. The number of epochs is another great place to play around. This is primarily due to wanting to avoid overfitting, which can happen by having too many training epochs. As such, should play around and see how many epochs yields the best result for the model.\n",
    "\n",
    "4. The optimizer function is another area to play around as RMSprop may not always be the best choice for the project at hand.\n",
    "\n",
    "5. The loss function selected here was another judgement call, but others could be used such as binary cross entropy. Used here since we actually have numbers to match to our model's prediction, so makes sense to use mean absolute error to see how far away our model's predictions are so we can mitigate the errors. Could also use root mean square method as well for the same purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that we have trained and validated our model, it is imperative we now test it. This will be done using model.evaluate() and the test data we set aside for it earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(test_gen)\n",
    "\n",
    "#Print out the accuracy of our model on the testing data\n",
    "\n",
    "print(\"Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Output\n",
    "\n",
    "Will now plot our model's prediction against the actual data, and validation data to see if we're overfitting, and how our model is performing overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will grab our losses by going into the training history and defining appropriate variables to make plotting easier\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Now plot our training and validation losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label = 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Training and Validation Losses By Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Will now have our model make a prediction\n",
    "# X will be our input that we give to our model to make its prediction\n",
    "\n",
    "prediction = model.predict(X)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "As is evident from the plot and historical output above, our model is able to achieve accuracy in the range of _%-_%. Furthermore, it is evident that in using _ epochs, the model is able to avoid overfitting to the training data. As such, it can be concluded that our model is sufficiently effective in predicting future asset growth for Canadian Credit Unions using Yelp! reviews, and yearly asset growth. Lastly, it can be concluded that Vancity Credit Uni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
