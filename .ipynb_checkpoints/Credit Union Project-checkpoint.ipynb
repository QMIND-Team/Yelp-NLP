{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canadian Credit Union Yelp and Asset Growth Project\n",
    "\n",
    "Here we explore the correlation between Yelp! reviews and asset growth using various recurrent neural networks. From our experimentation with the data, and features, we found _ yielded the best accuracy of future asset growth. This prediction was generated using Yelp! reviews and sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-Up\n",
    "\n",
    "Here we will import the necessary libraries that we will need for the project. Additionally, will read in the data collected from various Canadian Credit Unions and their corresponding Yelp! reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first start by importing the libraries and data we'll need\n",
    "# Libraries needed include numpy, keras, matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import SentimentAverages as sa\n",
    "\n",
    "# Import data\n",
    "\n",
    "data = pickle.load(open(\"dfFinal.p\", \"rb\"))\n",
    "\n",
    "print(data)\n",
    "\n",
    "cudata = pd.read_excel(\"cuinfo.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n",
    "\n",
    "Structure the data correctly as to be easily used by the generator and the keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cudata.keys()))\n",
    "companies = list(data.columns.levels[0])\n",
    "print(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indecies = ['Vancity','Coast Capital Savings','Servus Credit Union','Meridian Credit Union','Affinity Federal Credit Union']\n",
    "\n",
    "print('Vancity')\n",
    "vancity = sa.getSentimentAves('Vancity')\n",
    "print(vancity)\n",
    "print('Coast Capital Savings')\n",
    "print(sa.getSentimentAves('Coast Capital Savings'))\n",
    "print('Servus Credit Union')\n",
    "print(sa.getSentimentAves('Servus Credit Union'))\n",
    "print('Meridian Credit Union')\n",
    "print(sa.getSentimentAves('Meridian Credit Union'))\n",
    "print('Affinity Federal Credit Union')\n",
    "print(sa.getSentimentAves('Affinity Federal Credit Union'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from excel\n",
    "finaldata = [0,1,2,3,4]\n",
    "for company in companies:\n",
    "    # Translate Names\n",
    "    if company == \"Vancity\":\n",
    "        header = \"Vancity\"\n",
    "        index = 0\n",
    "    elif company == \"Coast Capital Savings\":\n",
    "        header = \"Coast Capital\"\n",
    "        index = 1\n",
    "    elif company == \"Servus Credit Union\":\n",
    "        header = \"Servus\"\n",
    "        index = 2\n",
    "    elif company == \"Meridian Credit Union\":\n",
    "        header = \"Meridian\"\n",
    "        index = 3\n",
    "    elif company == \"Affinity Federal Credit Union\":\n",
    "        header = \"Affinity\"\n",
    "        index = 4\n",
    "    elif company == \"Assiniboine Credit Union\":\n",
    "        continue\n",
    "    # Extract from correct sheet\n",
    "    one = pd.concat([cudata[header][\"Year (Growth from X-1 to X)\"], cudata[header][\"Total Assets ($ CAD)\"]], axis=1)\n",
    "    #one.set_index(one.columns[0], inplace=True)\n",
    "    one = one.dropna()\n",
    "    one[one.columns[0]] = pd.to_numeric(one[one.columns[0]], downcast='integer')\n",
    "    one.set_index(one.columns[0], inplace=True)\n",
    "    \n",
    "    toset = pd.concat([one,sa.getSentimentAves(company)], axis=1)\n",
    "    \n",
    "    finaldata[index] = toset.dropna().values\n",
    "\n",
    "    print(header)\n",
    "    print(finaldata[index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analytics \n",
    "\n",
    "Here we will plot some of our data to see if we can see any obvious patterns. Best to do that before going right into the model creation so that we can ensure that the obvious patterns are indeed accounted for during that stage. Here will simply look at temperature versus time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store our Vancity asset data into a numpy array for convenience\n",
    "\n",
    "# vassets  = float_data[:,1] \n",
    "# time  = data[:,1]\n",
    "# plt.plot(time, data, label = 'Asset Growth ($ CAD)')\n",
    "# plt.title('Asset Growth By Year')\n",
    "# plt.legend()\n",
    "# plt.xlabel('Year')\n",
    "# plt.ylabel('Asset Growth ($ CAD)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the plot above, we should be expecting year over year increase in assets. This will provide us with a diagnostic to ensure the model is providing us with reasonable output. Additionally, should get yearly growths in excess of a few hundreds of millions of dollars. This should pick up to a few billion dollars around the year 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "Will now define a generator function to create our training, validation and testing data sets for our model. Given the nature of the data we are working with, will require more hardcoding than might typically be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the generator.  Hardcoded for our purpose\n",
    "\n",
    "def train_gen(data):\n",
    "    lookback = 5\n",
    "    company = 0 #Counter for which company is being chosen\n",
    "    i = 0 # Index in that company's dataframe\n",
    "    \n",
    "    while 1:\n",
    "        while i + lookback < len(data[company]):\n",
    "            samples = np.zeros((1,lookback,2))\n",
    "            targets = np.zeros((1,))\n",
    "            for k, j in enumerate(range(i, i + lookback)):\n",
    "                # k in [0, lookback)\n",
    "                # j in [i, i + lookback)\n",
    "                for s in range(0,len(data[company][j])):\n",
    "                    samples[0][k][s] = data[company][j][s]\n",
    "            targets[0] = data[company][i + lookback][1]\n",
    "            i = i + 1\n",
    "            yield samples, targets\n",
    "        i = 0\n",
    "        if company < len(data) - 1:\n",
    "            company += 1\n",
    "        else:\n",
    "            company = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generator(data, lookback, delay , min_index, max_index, batch_size = 128):\n",
    "    step = 1\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data)  - delay - 1\n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        if i + batch_size >= max_index:\n",
    "            i = min_index + lookback\n",
    "        rows = np.arange(i, min(i + batch_size, max_index))\n",
    "        i += len(rows)\n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows), ))\n",
    "        \n",
    "        for j , row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,5):\n",
    "#     companytest = np.random.rand(25,3)\n",
    "#     datatest.append(companytest)\n",
    "# print(len(datatest), datatest[0].shape)\n",
    "\n",
    "train_data = list(finaldata[i] for i in [0,1,3] )\n",
    "#print(train_data)\n",
    "\n",
    "val_data = [finaldata[2]]\n",
    "#print(val_data)\n",
    "\n",
    "test_data = [finaldata[4]]\n",
    "print([test_data])\n",
    "\n",
    "\n",
    "train = train_gen(train_data)\n",
    "val = train_gen(val_data)\n",
    "test = train_gen(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(next(train)[0]), '\\n')\n",
    "print(next(val), '\\n')\n",
    "print(next(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "Finally, after all the hard work of normalizing our data, taking a quick look at it, we get the to fun part: model creation. Will be taking Yelp! reviews and provided asset growth data to predict future asset growth as our only output. This will be accomplished using an LSTM recurrent neural network model with _ layers and _ neurons. These were selected as they yielded the highest accuracy from our experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Need to import some libraries from keras to create our model\n",
    "# This will involve the use of keras sequential neural network models, layers and rmsprop optimizers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# The rest is very similar to the creation of the sequential neural network we made during the Warm-Up project\n",
    "# Quick refresher though, need to define our model with a number of layers, an optimzer function, a loss function, an activation function, and how many layers we want it to be\n",
    "\n",
    "# Define our model as a sequential one\n",
    "model = Sequential()\n",
    "\n",
    "# Add some layers to our model \n",
    "#model.add(LSTM(5, input_shape=(5, 2), return_sequences = True))\n",
    "model.add(layers.Flatten(input_shape=(5,2)))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Now compile our model with optimizer and loss functions, no metric for this one though\n",
    "model.compile(optimizer = RMSprop(), loss = 'mae')\n",
    "history =  model.fit_generator(train,\n",
    "                               epochs = 3,\n",
    "                               steps_per_epoch = 5,\n",
    "                               validation_data = val,\n",
    "                               validation_steps = 1)\n",
    "\n",
    "# Before we go any further, some important notes to make here. Will do that below in the \"Model Notes\" block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Notes\n",
    "\n",
    "1. Could have used other activation functions, 'relu' is a pretty popular one, but could use the likes of 'selu' and 'sigmoid'.\n",
    "\n",
    "2. The number of layers we add is completely arbitrary and is usually driven by experimenting with the model to see what works the best for the project.\n",
    "\n",
    "3. The number of epochs is another great place to play around. This is primarily due to wanting to avoid overfitting, which can happen by having too many training epochs. As such, should play around and see how many epochs yields the best result for the model.\n",
    "\n",
    "4. The optimizer function is another area to play around as RMSprop may not always be the best choice for the project at hand.\n",
    "\n",
    "5. The loss function selected here was another judgement call, but others could be used such as binary cross entropy. Used here since we actually have numbers to match to our model's prediction, so makes sense to use mean absolute error to see how far away our model's predictions are so we can mitigate the errors. Could also use root mean square method as well for the same purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Now that we have trained and validated our model, it is imperative we now test it. This will be done using model.evaluate() and the test data we set aside for it earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate_generator(test, steps = 3)\n",
    "\n",
    "#Print out the accuracy of our model on the testing data\n",
    "\n",
    "print(\"Accuracy:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Output\n",
    "\n",
    "Will now plot our model's prediction against the actual data, and validation data to see if we're overfitting, and how our model is performing overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will grab our losses by going into the training history and defining appropriate variables to make plotting easier\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Now plot our training and validation losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label = 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Training and Validation Losses By Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will now have our model make a prediction\n",
    "# X will be our input that we give to our model to make its prediction\n",
    "\n",
    "prediction = model.predict_generator(test, steps = 1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "As is evident from the plot and historical output above, our model is able to achieve accuracy in the range of _%-_%. Furthermore, it is evident that in using _ epochs, the model is able to avoid overfitting to the training data. As such, it can be concluded that our model is sufficiently effective in predicting future asset growth for Canadian Credit Unions using Yelp! reviews, and yearly asset growth. Lastly, it can be concluded that Vancity Credit Uni"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
