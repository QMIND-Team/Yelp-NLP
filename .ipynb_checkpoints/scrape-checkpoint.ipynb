{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Webscraper module.  Everything we need to scrape the Yelp data, putting everything we want into a Pandas dataframe.\n",
    "\n",
    "These functions should be imported by our main program.\n",
    "\n",
    "Following for now:\n",
    "https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
    "'''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "def getPage(url):\n",
    "    ''' Get the page object\n",
    "    Using the requests library.\n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    return page\n",
    "\n",
    "def openSoup(page):\n",
    "    ''' Make the soup object.\n",
    "    Takes a page object, preferably made by getPage().\n",
    "    '''\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "# def addToRevsList(revs, baseurl):\n",
    "#     ''' Add to the reviews, list version\n",
    "#     Take in the reviews list and the base url to scrape\n",
    "#     returns an extended reviews list, with the new reviews added\n",
    "#     '''\n",
    "#     stop = False\n",
    "#     index = 0\n",
    "#     numrevs = -1\n",
    "\n",
    "#     while not stop:\n",
    "#         url = baseurl + \"?start=\" + str(index)\n",
    "#         page = getPage(url)\n",
    "#         soup = openSoup(page)\n",
    "\n",
    "#         # If we havent figured out yet, get the max number of reviews from the top of the page\n",
    "#         # Note: This lies!\n",
    "#         if numrevs == -1:\n",
    "#             maxes = soup.find_all('span', class_=\"review-count rating-qualifier\")\n",
    "#             tmpstring = maxes[0].get_text()\n",
    "#             # This next line turns the string \"64 reviews\" into the int \"64\"\n",
    "#             numrevs = int(re.findall(r'\\d+', tmpstring)[0]) #Yeah that could be broken to a few lines for readability\n",
    "\n",
    "#         lists = soup.find_all('div', class_=\"review-content\")\n",
    "#         parlist = [l.find_all('p') for l in lists]\n",
    "\n",
    "#         # parlist is a list of (single item) lists of p blocks\n",
    "#         pars = []\n",
    "#         for plist in parlist:\n",
    "#             for p in plist:\n",
    "#                 pars.append(p)\n",
    "\n",
    "#         revs.extend([p.get_text() for p in pars]) # Extend, not append here\n",
    "\n",
    "#         index += 20\n",
    "\n",
    "#         if index >= numrevs:\n",
    "#             # Not sure if the best way is to do this, or to make this a break and have the while loop be an infinite loop.\n",
    "#             stop = True\n",
    "    \n",
    "#     return revs\n",
    "\n",
    "def extractRevs(soup):\n",
    "    '''TODO\n",
    "    Docstring\n",
    "    '''\n",
    "    lists = soup.find_all('div', class_=\"review-content\")\n",
    "    parlist = [l.find_all('p') for l in lists]\n",
    "    \n",
    "    pars = []\n",
    "    for plist in parlist:\n",
    "        for p in plist:\n",
    "            pars.append(p.get_text())\n",
    "\n",
    "    return pars\n",
    "\n",
    "def extractDates(soup):\n",
    "    ''' TODO\n",
    "    Docstring\n",
    "    '''\n",
    "    dates = soup.find_all('span', class_=\"rating-qualifier\")\n",
    "    datelistraw = [d.get_text().strip() for d in dates]\n",
    "\n",
    "    for date in list(datelistraw):\n",
    "        if \"reviews\" in date:\n",
    "            datelistraw.remove(date)\n",
    "\n",
    "    datelist = []\n",
    "\n",
    "    for date in datelistraw:\n",
    "        match = re.search(r'\\d{1,2}/\\d{1,2}/\\d{4}', date).group(0)\n",
    "        datelist.append(match)\n",
    "    \n",
    "    return datelist\n",
    "\n",
    "#Get the star rating from the review\n",
    "def extractStars(soup):\n",
    "    ''' TODO\n",
    "    Docstring\n",
    "    '''\n",
    "    stars = soup.find_all('class', class_=\"star-selector_label\")\n",
    "    starslist = [s.get_text().strip() for s in stars]\n",
    "    print(starslist)\n",
    "    return starslist\n",
    "\n",
    "\n",
    "def addToRevs(revs, baseurl):\n",
    "    '''Add to the reviews, dataframe version\n",
    "    Take in the reviews list and the base url to scrape\n",
    "    returns an extended reviews list, with the new reviews added\n",
    "    '''\n",
    "    stop = False\n",
    "    index = 0\n",
    "    numrevs = -1\n",
    "\n",
    "    while not stop:\n",
    "        url = baseurl + \"?start=\" + str(index)\n",
    "        page = getPage(url)\n",
    "        soup = openSoup(page)\n",
    "\n",
    "        # If we havent figured out yet, get the max number of reviews from the top of the page\n",
    "        # Note: This lies!\n",
    "        if numrevs == -1:\n",
    "            maxes = soup.find_all('span', class_=\"review-count rating-qualifier\")\n",
    "            tmpstring = maxes[0].get_text()\n",
    "            # This next line turns the string \"64 reviews\" into the int \"64\"\n",
    "            numrevs = int(re.findall(r'\\d+', tmpstring)[0]) #Yeah that could be broken to a few lines for readability\n",
    "\n",
    "        pars = extractRevs(soup)\n",
    "        dates = extractDates(soup)\n",
    "        stars = extractStars(soup)\n",
    "\n",
    "        df = pd.DataFrame(pars,dates, stars, columns = [\"Reviews\", \"Dates\",\"Stars\"]) #saves 20 reviews as dataframe\n",
    "        revs = pd.concat([df,revs], ignore_index= True, sort = True) #adds to our main dataframe, avoids index from repeatedly going 0-19\n",
    "\n",
    "        index += 20\n",
    "\n",
    "        if index >= numrevs:\n",
    "            # Not sure if the best way is to do this, or to make this a break and have the while loop be an infinite loop.\n",
    "            stop = True\n",
    "    \n",
    "    return revs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
